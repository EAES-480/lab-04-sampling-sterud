---
title: "Lab 04 — Sampling from Time Series"
author: "EAES 480 — Modern Statistics in Earth & Environmental Science"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

# Overview

## What is AmeriFlux?

**AmeriFlux** is a network of **eddy covariance** flux-tower sites that measure exchanges of **carbon (CO₂), water, and energy** between ecosystems and the atmosphere, with standardized data products shared for research and education.

In this lab, you will use a simplified AmeriFlux-style dataset from the **US-AMS site at Argonne National Laboratory (near Chicago)**. The measurements are at **30-minute resolution** over **2023**, and show strong **seasonality** and **day–night cycles**.

**Key idea for this lab:** treat the full 2023 time series as the **population**, then practice sampling strategies to estimate population parameters.

References for context:
- AmeriFlux overview: https://ameriflux.lbl.gov/about/about-ameriflux/
- US-AMS site page: https://ameriflux.lbl.gov/sites/siteinfo/US-AMS

---

# Learning goals

By the end of this lab, you should be able to:

- Define a **population** and a **sample** for an EAES time-series dataset
- Compute population **parameters** (mean, SD) and compare to sample **estimates**
- Visualize distributions and identify **latent grouping variables** (month, day/night)
- Implement **simple random sampling** and **stratified sampling**
- Use `set.seed()` to make sampling reproducible

---

# Data

## Load and inspect

```{r load_data, echo=TRUE, eval=FALSE}
df <- read_csv("data/us-ams-simple.csv") %>%
  clean_names() %>% filter(year_local == 2023)

glimpse(df)
```

**CHECK:** You should see columns like `year_local`, `doy`, `daytime`, and flux/biomet variables (e.g., `gpp`, `fc`, `le`, `ta`).

---

## Create a date and month column from DOY

This dataset uses **Year + Day-of-Year (DOY)**. Month must be derived from a calendar date.

```{r derive_time, echo=TRUE, eval=T}
df <- df %>%
  mutate(
    # TODO: create a Date column from year_local and doy
    # HINT: Jan 1 is DOY = 1, so use (doy - 1) with origin = "YYYY-01-01"
    date = as.Date(doy - 1, origin = paste0(year_local, "-01-01")),

    # TODO: create a month column (numeric 1–12 or labeled months)
    month = month(date, label = TRUE, abbr = TRUE),

    # TODO: make a day/night label using daytime (0/1)
    day_night = if_else(daytime == 1, "Day", "Night")
  )

count(df, month)
count(df, day_night)

```

---

# Choose a response variable

You will analyze **one response variable** throughout the lab. This could be a CO₂ flux metric or a meteorological variable.

Examples you can choose from (depending on what you see in the dataset):
- CO₂ / carbon: `gpp`, `reco`, `fc`
- Energy: `le`
- Meteorology: `ta`, `ts`, `swc`

```{r choose_response, echo=TRUE, eval=T}
# TODO: choose ONE response variable (a column name as a string)
response_var <- "reco"   # replace "gpp" with your choice, e.g. "fc" or "le" or "ta"

# CHECK: print a quick summary
df %>% summarise(
  n = n(),
  n_missing = sum(is.na(reco)),
  mean = mean(ta, na.rm = TRUE),
  sd = sd(ta, na.rm = TRUE)
)
```

**Prompt (2–3 sentences):** Why did you choose this response variable? What do you expect its seasonality/day–night pattern to be?

> *'TA' is Atmospheric Temperature. I expect that it will increase in summer, decrease in winter, be higher in the middle day, and lower at the end of the night*

---

# Section 1 — Data dictionary (conceptual)

Students will populate the data dictionary using:
https://ameriflux.lbl.gov/data/aboutdata/data-variables/

Fill in at least **5 variables** from this dataset:

| Variable | Units | Description | Expected sign/seasonality? |
|----------|-------|-------------|----------------------------|
|    ta      |  C  |     Atmospheric Temperature      |  summer (+), winter (-)       |
|    le      |   W m-2   |   latent heat turbulent flux      |   summer (+), winter (-) |
|    gpp      |  µmolCO2 m-2 s-1   | Gross Primary Productivity  |    summer (+), winter (-) |
|     fc     |   µmolCO2 m-2 s-1    |Carbon Dioxide (CO2) turbulent flux (no storage correction)| summer (+), winter (-) |
|     reco     |   µmolCO2 m-2 s-1   |   Ecosystem Respiration    |   summer (+), winter (-)  |

---

# Section 2 — Visualizing the population

Remember: for this lab, the **population** is the entire 2023 half-hourly time series.

## 2.1 Time series view

```{r plot_time_series, echo=TRUE, eval=T}
# GOAL: Visualize seasonality over the year.
# TODO: pick a y aesthetic using response_var.

df %>%
  filter(year_local == 2023, daytime != -9999, doy != -9999) %>%
  ggplot(aes(x = date, y = reco)) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(
    x = NULL,
    y = "Ecosystem Respiration (reco)",
    title = "Population time series (2023)"
  )
```

**Prompt (2–3 sentences):** What major patterns do you see? (Seasonal cycle? Daily cycle? Outliers?)

> *In the warmer months (June-September), there is higher ecosystem respiration, and there are more volatile fluxes. In the cooler months (October-May), the respiration is lower, and it is more stable.*

---

## 2.2 Population distribution (histogram + density)

```{r pop_distribution, echo=TRUE, eval=T}
# GOAL: See the overall distribution of the population.
# TODO: choose an appropriate number of bins (start with ~50).

ggplot(df, aes(x = reco)) +
  geom_histogram(bins = 12, alpha = 0.7) +
  theme_classic(base_size = 18) +
  labs(x = "Ecosystem Respiration (reco)", y = "Count", title = "Population distribution (histogram)")

ggplot(df, aes(x = reco)) +
  geom_density(alpha = 0.7) +
  theme_classic(base_size = 18) +
  labs(x = "Ecosystem Respiration (reco)", y = "Density", title = "Population distribution (density)")
```

**Prompt:** Describe shape (skew, modality), center, and spread.

> *The histogram is right-skewed, and it appears bimodal, with one peak at 2, and the other around 10. The largest number of points occur between 0-5 µmolCO2 m-2 s-1 of Ecosystem Respiration.The spread is from -1 to about 22 units.*

---

## 2.3 Do latent groups explain variability? (month, day/night)

### By month

```{r pop_by_month, echo=TRUE, eval=T}
# TODO: make month appear in a sensible order (it already is an ordered factor if label=TRUE)
df %>%
  ggplot(aes(x = month, y = reco)) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by month")
```

### By day/night

```{r pop_by_daynight, echo=TRUE, eval=FALSE}
df %>%
  ggplot(aes(x = day_night, y = reco)) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by day vs night")
```

**Prompt (3–4 sentences):** Which grouping variable (month or day/night) seems to explain more variability in your response? Why?

> *The monthly grouping variable explains more variability because the change in reco is more extreme.The box plot's IQR range is a lot more dramatic when comparing, for example, January to May. Although day and night reco change, they are less intense than the changes in Ecosystem Respiration Monthly.*

---

# Section 3 — Population parameters (truth)

Compute the population mean and SD for your chosen response variable.

```{r population_params, echo=TRUE, eval=FALSE}
pop_mean <- mean(df$reco, na.rm = TRUE)
pop_sd   <- sd(df$reco, na.rm = TRUE)

tibble(
  response_var = response_var,
  population_mean = pop_mean,
  population_sd = pop_sd
)
```

---

# Section 4 — Simple random sampling (SRS)

## 4.1 One random sample

```{r one_sample, echo=TRUE, eval=FALSE}
set.seed(480)

# TODO: choose a sample size (e.g., 200, 500, 1000)
n_samp <- 500

samp <- df %>%
  slice_sample(n = n_samp)

samp_mean <- mean(samp$reco, na.rm = TRUE)
samp_sd   <- sd(samp$reco, na.rm = TRUE)

tibble(
  n_samp = n_samp,
  sample_mean = samp_mean,
  sample_sd = samp_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

**Prompt (2–3 sentences):** How close is your one-sample estimate to the population mean/SD? Is the difference surprising?

> *The one-sample estimate and the population mean/SD are very very similar. This difference is not suprising, because it shows that the data is likely pretty consistent yearly. There are already a lot of data variables creating the original data set, therefore sampling it many times does not change the results that greatly.*

---

## 4.2 Sampling variability: many samples → many means

```{r sampling_distribution, echo=TRUE, eval=FALSE}
set.seed(480)

reps <- 1500   # TODO: choose number of replicates (e.g., 500 or 1000)

means <- replicate(
  reps,
  df %>%
    slice_sample(n = n_samp) %>%
    summarise(m = mean(reco, na.rm = TRUE)) %>%
    pull(m)
)

ggplot(tibble(mean_est = means), aes(mean_est)) +
  geom_histogram(bins = 40, alpha = 0.8) +
  geom_vline(xintercept = pop_mean, linetype = "dashed", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = paste0("Sample mean of ", response_var),
    y = "Count",
    title = "Sampling distribution of the mean (SRS)",
    subtitle = "Dashed line = population mean"
  )
```

**Prompt (2–3 sentences):** Is the sampling distribution centered on the population mean? What happens if you increase `n_samp`?

> *Yes, the sampling distribution is centered on the population mean. If you increase the number of samples, there is more flattened out peak that still exists at the center of the population mean. This means that more and more variables are near that sample mean.*

---

# Section 5 — Stratified sampling

Here you’ll test whether stratification helps when the population has structure.

## 5.1 Stratify by month

```{r strat_by_month, echo=TRUE, eval=FALSE}
set.seed(480)

# GOAL: sample within each month to ensure seasonal representation.
# TODO: choose n_per_month so total sample size is reasonable (e.g., 12 * 20 = 240)
n_per_month <- (12 * 30)

samp_strat <- df %>%
  group_by(month) %>%
  slice_sample(n = n_per_month) %>%
  ungroup()

strat_mean <- mean(samp_strat$reco, na.rm = TRUE)
strat_sd   <- sd(samp_strat$reco, na.rm = TRUE)

tibble(
  n_per_month = n_per_month,
  total_n = nrow(samp_strat),
  strat_mean = strat_mean,
  strat_sd = strat_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

---

## 5.2 Compare strategies (SRS vs stratified)

```{r compare_sampling, echo=TRUE, eval=FALSE}
tibble(
  strategy = c("Population", "SRS", "Stratified by month"),
  mean = c(pop_mean, samp_mean, strat_mean),
  sd   = c(pop_sd,   samp_sd,   strat_sd)
)
```

**Prompt (3–4 sentences):** Which strategy better approximated the population mean and SD for your response variable? Why might stratification help (or not) here?

> *The SRS better approximated the population mean, because the value compared to population is more similar. Although the sd is lower, that just shows that the values have an even better fit. Stratification helps to ensure representation of each month, however when looking at a huge dataset like the one given, it isn't as important.*

---

# Section 6 — Conceptual reflection

Answer in **4–6 sentences**:

- Why does seasonality matter for sampling?
- What happens if sampling ignores latent grouping variables?
- In EAES field studies, when is stratification essential?
- What is one trade-off of stratified sampling?

> *Seasonality matters when looking at environmental data because data values can vary greatly throughout each month. If sampling ignores latent grouping variables, the means and medians can be shifted to favor the groups with larger densities of values. Stratification is essential to properly represent every different group present in the data set, for example this data set is biased towards the summer months if not stratified. One trade-off with the stratified sampling is that it can over-represent certian groups if they are not divied up correctly (every month has a different amount of days, saying every month is 30 days could affect stratified results).*

---

# Part II — Sampling designs extensions (graded practice)

In Part I you treated the full 2023 half-hourly record as a **population**, then compared **simple random sampling** (SRS) vs **stratified sampling by month**.

Now you will practice additional **sampling designs** discussed in lecture:

- **Systematic** sampling (regular interval)
- **Cluster** sampling (sample groups, then measure everything in them)
- **Quasi-continuous** sampling (regular time series subsampling)
- **Blocked** designs (preview only — think “blocks as structure”)

All exercises below should run using the same objects from Part I:
- `df` (with `date`, `month`, `day_night`)
- `response_var`
- `pop_mean`, `pop_sd`
- your SRS sample `samp` and stratified sample `samp_strat` (if you created them)

> **Tip:** If you renamed objects in Part I, update the code below to match your names.

---

## Exercise 1 — Systematic sampling (every k-th observation)

**Idea:** sample at a fixed interval (e.g., every 48th record ≈ daily at 30-min resolution).

**Risk:** if the variable has strong cycles aligned with the interval, systematic sampling can be biased.

```{r systematic_sampling, echo=TRUE, eval=FALSE}
# GOAL: Create a systematic sample and compare mean/sd to population.
# TODO: choose an interval k (try 48, 24, 96).
k <- ___

sys_samp <- df %>%
  # TODO: keep only non-missing response values
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = k))

sys_mean <- mean(sys_samp[[response_var]], na.rm = TRUE)
sys_sd   <- sd(sys_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Systematic"),
  mean = c(pop_mean, sys_mean),
  sd   = c(pop_sd,   sys_sd),
  n    = c(nrow(df), nrow(sys_samp))
)
```

**Prompt (3–4 sentences):** Did systematic sampling approximate the population mean/SD better or worse than SRS?  
What cycle (daily or seasonal) might be interacting with your chosen `k`?

> *Write your answer here.*

---

## Exercise 2 — Cluster sampling (sample days, take all points within those days)

**Definition:** choose a set of clusters (here, **days**) at random, then include **all observations** in the chosen clusters.

This mimics EAES logistics: you may only be able to sample on certain days.

```{r cluster_sampling_days, echo=TRUE, eval=FALSE}
# GOAL: Sample whole days as clusters, then estimate mean/sd.
# TODO: choose number of days to sample.
set.seed(480)

n_days <- ___

days <- df %>%
  distinct(date) %>%
  drop_na(date) %>%
  pull(date)

chosen_days <- sample(days, size = n_days, replace = FALSE)

cluster_samp <- df %>%
  filter(date %in% chosen_days) %>%
  filter(!is.na(.data[[response_var]]))

clust_mean <- mean(cluster_samp[[response_var]], na.rm = TRUE)
clust_sd   <- sd(cluster_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Cluster (days)"),
  mean = c(pop_mean, clust_mean),
  sd   = c(pop_sd,   clust_sd),
  n    = c(nrow(df), nrow(cluster_samp))
)
```

**Prompt (3–4 sentences):** Why might cluster sampling have **higher variance** than SRS for the same number of measurements?  
What feature of time series data (hint: autocorrelation) is relevant here?

> *Write your answer here.*

---

## Exercise 3 — Quasi-continuous sampling (fixed schedule time series)

**Definition:** sample regularly over time to create a *subsampled time series*.

This mimics continuous instrumentation that logs at a lower frequency (e.g., hourly instead of 30-min).

```{r quasi_continuous, echo=TRUE, eval=FALSE}
# GOAL: Create a regular subsampled time series.
# TODO: choose a step size (every 2 records = hourly; every 4 = 2-hourly).
step <- ___

qc_samp <- df %>%
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = step))

qc_mean <- mean(qc_samp[[response_var]], na.rm = TRUE)
qc_sd   <- sd(qc_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Quasi-continuous"),
  mean = c(pop_mean, qc_mean),
  sd   = c(pop_sd,   qc_sd),
  n    = c(nrow(df), nrow(qc_samp))
)
```

### Visual check: does the subsampled series preserve structure?

```{r qc_plot, echo=TRUE, eval=FALSE}
# TODO: plot BOTH population and qc_samp time series (thin lines) for a short window
# HINT: filter to one month to avoid overplotting.

df %>%
  filter(month == ___) %>%   # e.g., "Jul" if month is labeled
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(title = "Population time series (subset)", x = NULL, y = response_var)

qc_samp %>%
  filter(month == ___) %>%
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.6) +
  theme_classic(base_size = 18) +
  labs(title = "Quasi-continuous sample time series (subset)", x = NULL, y = response_var)
```

**Prompt (3–4 sentences):** Does quasi-continuous sampling preserve the **seasonal** pattern? The **daily** pattern?  
In an EAES context, when is quasi-continuous sampling preferable to sparse discrete sampling?

> *Write your answer here.*

---

## Exercise 4 — Compare all strategies in one table (and interpret)

```{r compare_all, echo=TRUE, eval=FALSE}
# GOAL: Assemble a comparison table of mean/sd error for each strategy.
# NOTE: This assumes you created objects: samp, samp_strat, sys_samp, cluster_samp, qc_samp.
# If your object names differ, update them here.

summ_stats <- function(dat, label) {
  tibble(
    strategy = label,
    n = nrow(dat),
    mean = mean(dat[[response_var]], na.rm = TRUE),
    sd   = sd(dat[[response_var]], na.rm = TRUE)
  )
}

bind_rows(
  tibble(strategy = "Population", n = nrow(df), mean = pop_mean, sd = pop_sd),
  summ_stats(samp, "SRS"),
  summ_stats(samp_strat, "Stratified (month)"),
  summ_stats(sys_samp, "Systematic"),
  summ_stats(cluster_samp, "Cluster (days)"),
  summ_stats(qc_samp, "Quasi-continuous")
) %>%
  mutate(
    mean_error = mean - pop_mean,
    sd_error   = sd - pop_sd
  )
```

**Prompt (5–6 sentences, graded):** Which strategy gave the best estimate of the population mean? Of the population SD?  
Explain *why* in terms of (i) seasonal/diurnal structure and (ii) dependence/autocorrelation.  
If you were designing a real EAES study with limited field days, what hybrid strategy would you propose (e.g., stratified + clustered)?

> *Write your answer here.*

---

## Blocked designs (preview only — do not implement yet)

A **blocked** design means sampling within blocks (time blocks like months, or spatial blocks like sites), then later treating block as structure in the model (often as a random effect).

You already approximated blocking by stratifying over `month`. Later, we will return to this idea when we fit models that include block structure explicitly.

# Submission + self-check

## Before you knit (Run All)
- [ ] Setup chunk runs without errors
- [ ] Data join happened (df_raw has both canopy + climate columns)
- [ ] All chunks run top-to-bottom
- [ ] No objects created only in the Console
- [ ] Interpretation answers are complete sentences

## After you knit
- [ ] Figures appear in the output
- [ ] Tables render and are readable
- [ ] Your selected x and y are clearly stated in the document

**Save, commit, push to Github.**
